{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5136089,"sourceType":"datasetVersion","datasetId":2983767},{"sourceId":9551246,"sourceType":"datasetVersion","datasetId":5819466}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = '/kaggle/input/disease-dataset1/dataset.csv'\ndataset = pd.read_csv(file_path)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Identify symptom columns (assumed to be all columns except 'Disease')\nsymptom_columns = dataset.columns[1:]  # Assuming 'Disease' is in the first column\n\n# Function to sort symptoms alphabetically (treating NaN as 'Unknown')\ndef sort_symptoms(row):\n    symptoms = row[symptom_columns].fillna('Unknown').astype(str).values  # Convert to string and handle NaN\n    symptoms_sorted = sorted(symptoms)  # Sort symptoms alphabetically\n    return pd.Series(symptoms_sorted, index=symptom_columns)\n\n# Apply the sorting function to each row\nsorted_symptoms_df = dataset.apply(sort_symptoms, axis=1)\n\n# Combine sorted symptoms with the target variable 'Disease'\nsorted_data = pd.concat([sorted_symptoms_df, dataset['Disease']], axis=1)\n\n# Display the first few rows of the sorted dataset\nprint(sorted_data.head())\n\n# Optionally, save the sorted dataset to a new CSV file\n# sorted_data.to_csv('sorted_symptom_dataset.csv', index=False)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport json\n\n# open the csv file\nwith open(\"/kaggle/input/disease-dataset1/Symptom-severity.csv\") as f:\n    reader = csv.reader(f)\n    # skip the header\n    next(reader)\n    # create a list to store the data\n    data = []\n    # iterate over the rows\n    for i,row in enumerate(reader):\n        # create a dictionary to store the data\n        d = {}\n        # store the data in the dictionary\n        d['serial'] = i\n        d['name'] = row[0]\n        d['weight'] = row[1]\n        # append the dictionary to the list\n        data.append(d)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filling missing symptom values with a placeholder 'Unknown'\nimputer = SimpleImputer(strategy='constant', fill_value='Unknown')\nsymptoms = sorted_data.iloc[:, 1:]  # Selecting all symptom columns\nsymptoms_filled = pd.DataFrame(imputer.fit_transform(symptoms), columns=symptoms.columns)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_data.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding categorical features (symptoms)\nencoder = {'itching': 0, 'skin_rash': 1, 'nodal_skin_eruptions': 2, 'continuous_sneezing': 3, 'shivering': 4, 'chills': 5, 'joint_pain': 6, 'stomach_pain': 7, 'acidity': 8, 'ulcers_on_tongue': 9, 'muscle_wasting': 10, 'vomiting': 11, 'burning_micturition': 12, 'spotting_urination': 13, 'fatigue': 14, 'weight_gain': 15, 'anxiety': 16, 'cold_hands_and_feets': 17, 'mood_swings': 18, 'weight_loss': 19, 'restlessness': 20, 'lethargy': 21, 'patches_in_throat': 22, 'irregular_sugar_level': 23, 'cough': 24, 'high_fever': 25, 'sunken_eyes': 26, 'breathlessness': 27, 'sweating': 28, 'dehydration': 29, 'indigestion': 30, 'headache': 31, 'yellowish_skin': 32, 'dark_urine': 33, 'nausea': 34, 'loss_of_appetite': 35, 'pain_behind_the_eyes': 36, 'back_pain': 37, 'constipation': 38, 'abdominal_pain': 39, 'diarrhoea': 40, 'mild_fever': 41, 'yellow_urine': 42, 'yellowing_of_eyes': 43, 'acute_liver_failure': 44, 'fluid_overload': 117, 'swelling_of_stomach': 46, 'swelled_lymph_nodes': 47, 'malaise': 48, 'blurred_and_distorted_vision': 49, 'phlegm': 50, 'throat_irritation': 51, 'redness_of_eyes': 52, 'sinus_pressure': 53, 'runny_nose': 54, 'congestion': 55, 'chest_pain': 56, 'weakness_in_limbs': 57, 'fast_heart_rate': 58, 'pain_during_bowel_movements': 59, 'pain_in_anal_region': 60, 'bloody_stool': 61, 'irritation_in_anus': 62, 'neck_pain': 63, 'dizziness': 64, 'cramps': 65, 'bruising': 66, 'obesity': 67, 'swollen_legs': 68, 'swollen_blood_vessels': 69, 'puffy_face_and_eyes': 70, 'enlarged_thyroid': 71, 'brittle_nails': 72, 'swollen_extremeties': 73, 'excessive_hunger': 74, 'extra_marital_contacts': 75, 'drying_and_tingling_lips': 76, 'slurred_speech': 77, 'knee_pain': 78, 'hip_joint_pain': 79, 'muscle_weakness': 80, 'stiff_neck': 81, 'swelling_joints': 82, 'movement_stiffness': 83, 'spinning_movements': 84, 'loss_of_balance': 85, 'unsteadiness': 86, 'weakness_of_one_body_side': 87, 'loss_of_smell': 88, 'bladder_discomfort': 89, 'foul_smell_ofurine': 90, 'continuous_feel_of_urine': 91, 'passage_of_gases': 92, 'internal_itching': 93, 'toxic_look_(typhos)': 94, 'depression': 95, 'irritability': 96, 'muscle_pain': 97, 'altered_sensorium': 98, 'red_spots_over_body': 99, 'belly_pain': 100, 'abnormal_menstruation': 101, 'dischromic_patches': 102, 'watering_from_eyes': 103, 'increased_appetite': 104, 'polyuria': 105, 'family_history': 106, 'mucoid_sputum': 107, 'rusty_sputum': 108, 'lack_of_concentration': 109, 'visual_disturbances': 110, 'receiving_blood_transfusion': 111, 'receiving_unsterile_injections': 112, 'coma': 113, 'stomach_bleeding': 114, 'distention_of_abdomen': 115, 'history_of_alcohol_consumption': 116, 'blood_in_sputum': 118, 'prominent_veins_on_calf': 119, 'palpitations': 120, 'painful_walking': 121, 'pus_filled_pimples': 122, 'blackheads': 123, 'scurring': 124, 'skin_peeling': 125, 'silver_like_dusting': 126, 'small_dents_in_nails': 127, 'inflammatory_nails': 128, 'blister': 129, 'red_sore_around_nose': 130, 'yellow_crust_ooze': 131, 'prognosis': 132}\nfor column in symptoms_filled.columns:\n    symptoms_filled[column] = symptoms_filled[column].map(encoder)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding the target 'Disease'\ndisease_encoded = encoder.fit_transform(sorted_data['Disease'])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combining the preprocessed features and target\npreprocessed_data = symptoms_filled.copy()\npreprocessed_data = preprocessed_data.reset_index(drop=True)\npreprocessed_data['Disease'] = disease_encoded","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first few rows of the preprocessed data\npreprocessed_data.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the dataset into features and target\nX = preprocessed_data.drop(columns=['Disease'])\ny = preprocessed_data['Disease']","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_classes = preprocessed_data['Disease'].nunique()\nn_classes","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization\n\n# Define the model architecture\nmodel = Sequential()\nmodel.add(Input(shape=(n_classes,), name='Input_Layer'))  # Use Input layer here\nmodel.add(Dense(256, activation='relu', name='Dense_Layer_1'))\nmodel.add(BatchNormalization(name='Batch_Normalization_1'))\nmodel.add(Dropout(0.5, name='Dropout_1'))\nmodel.add(Dense(128, activation='relu', name='Hidden_Layer'))\nmodel.add(BatchNormalization(name='Batch_Normalization_2'))\nmodel.add(Dropout(0.5, name='Dropout_2'))\nmodel.add(Dense(n_classes, activation='softmax', name='Output_Layer'))\n\n# Plot the model\nplot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n\n# Display the plot\nif os.path.exists('E:/medical_results_app/server/model/model_architecture.png'):\n    img = plt.imread('E:/medical_results_app/server/model/model_architecture.png')\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"File not found: 'model_architecture.png'. Please check if it was created successfully.\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network","metadata":{"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n\n# Check the shape of X_train to determine the number of features\nn_features = X_train.shape[1]  # This should be the number of input features\nn_classes = len(set(y_train))  # Assuming y_train contains class labels\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(n_features,), activation='relu'))  # Input layer\nmodel.add(BatchNormalization())  # Normalize the output of the previous layer\nmodel.add(Dropout(0.5))  # Dropout to reduce overfitting\n\nmodel.add(Dense(128, activation='relu'))  # Hidden layer\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(n_classes, activation='softmax'))  # Output layer","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To ignore warinings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train_one_hot = to_categorical(y_train, num_classes=n_classes)\ny_test_one_hot = to_categorical(y_test, num_classes=n_classes)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X_train, y_train_one_hot, epochs=30, batch_size=32, validation_data=(X_test, y_test_one_hot))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the label encoder for symptoms\njoblib.dump(encoder, 'symptom_encoder.joblib')\n\n# Save the label encoder for diseases (if different from symptoms)\njoblib.dump(encoder, 'disease_encoder.joblib')\n\n# Save the SimpleImputer used for handling missing values\njoblib.dump(imputer, 'imputer.joblib')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"joblib_file = 'model.joblib'\njoblib.dump(model, joblib_file)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Assuming you have X_test and y_test\nn_splits = 5\n\n# Calculate the size of each split\nsplit_size = len(X_test) // n_splits\n\n# Initialize lists to store evaluation metrics for each split\naccuracies = []\nprecisions = []\nrecalls = []\n\n# Loop through each split\nfor i in range(n_splits):\n    # Define the start and end indices for the split\n    start_idx = i * split_size\n    if i == n_splits - 1:\n        end_idx = len(X_test)  # Include the remainder in the last split\n    else:\n        end_idx = (i + 1) * split_size\n    \n    # Get the split for testing\n    X_split = X_test[start_idx:end_idx]\n    y_split = y_test[start_idx:end_idx]\n    \n    # Make predictions using the pre-trained model\n    y_pred = model.predict(X_split)\n    \n    # Convert predictions to class labels (assuming softmax probabilities)\n    y_pred_labels = np.argmax(y_pred, axis=1)\n    \n    # Evaluate performance\n    accuracies.append(accuracy_score(y_split, y_pred_labels))\n    precisions.append(precision_score(y_split, y_pred_labels, average='weighted'))\n    recalls.append(recall_score(y_split, y_pred_labels, average='weighted'))\n\n# Print the results\nprint(\"Accuracies for each split:\", accuracies)\nprint(\"Precisions for each split:\", precisions)\nprint(\"Recalls for each split:\", recalls)\n\n# Optionally, calculate average performance across splits\nprint(\"Average accuracy:\", np.mean(accuracies))\nprint(\"Average precision:\", np.mean(precisions))\nprint(\"Average recall:\", np.mean(recalls))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"testdf = pd.read_csv(\"/kaggle/input/medical-test-data/new_unseen_data.csv\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Identify symptom columns (assumed to be all columns except 'Disease')\nsymptom_columns = dataset.columns[1:]  # Assuming 'Disease' is in the first column\n\n# Function to sort symptoms alphabetically (treating NaN as 'Unknown')\ndef sort_symptoms(row):\n    symptoms = row[symptom_columns].fillna('Unknown').astype(str).values  # Convert to string and handle NaN\n    symptoms_sorted = sorted(symptoms)  # Sort symptoms alphabetically\n    return pd.Series(symptoms_sorted, index=symptom_columns)\n\n# Apply the sorting function to each row\nsorted_symptoms_df = dataset.apply(sort_symptoms, axis=1)\n\n# Combine sorted symptoms with the target variable 'Disease'\nsorted_data = pd.concat([sorted_symptoms_df, dataset['Disease']], axis=1)\n\n# Display the first few rows of the sorted dataset\nprint(sorted_data.head())\n\n# Optionally, save the sorted dataset to a new CSV file\n# sorted_data.to_csv('sorted_symptom_dataset.csv', index=False)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filling missing symptom values with a placeholder 'Unknown'\nimputer = SimpleImputer(strategy='constant', fill_value='Unknown')\nsymptoms = sorted_data.iloc[:, 1:]  # Selecting all symptom columns\nsymptoms_filled = pd.DataFrame(imputer.fit_transform(symptoms), columns=symptoms.columns)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding categorical features (symptoms)\nencoder = LabelEncoder()\nfor column in symptoms_filled.columns:\n    symptoms_filled[column] = encoder.fit_transform(symptoms_filled[column])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding the target 'Disease'\ndisease_encoded = encoder.fit_transform(sorted_data['Disease'])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combining the preprocessed features and target\npreprocessed_data = symptoms_filled.copy()\npreprocessed_data['Disease'] = disease_encoded","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the dataset into features and target\nX_test = preprocessed_data.drop(columns=['Disease'])\ny_test = preprocessed_data['Disease']","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Assuming you have X_test and y_test\nn_splits = 5\n\n# Calculate the size of each split\nsplit_size = len(X_test) // n_splits\n\n# Initialize lists to store evaluation metrics for each split\naccuracies = []\nprecisions = []\nrecalls = []\n\n# Loop through each split\nfor i in range(n_splits):\n    # Define the start and end indices for the split\n    start_idx = i * split_size\n    if i == n_splits - 1:\n        end_idx = len(X_test)  # Include the remainder in the last split\n    else:\n        end_idx = (i + 1) * split_size\n    \n    # Get the split for testing\n    X_split = X_test[start_idx:end_idx]\n    y_split = y_test[start_idx:end_idx]\n    \n    # Make predictions using the pre-trained model\n    y_pred = model.predict(X_split)\n    \n    # Convert predictions to class labels (assuming softmax probabilities)\n    y_pred_labels = np.argmax(y_pred, axis=1)\n    \n    # Evaluate performance\n    accuracies.append(accuracy_score(y_split, y_pred_labels))\n    precisions.append(precision_score(y_split, y_pred_labels, average='weighted'))\n    recalls.append(recall_score(y_split, y_pred_labels, average='weighted'))\n\n# Print the results\nprint(\"Accuracies for each split:\", accuracies)\nprint(\"Precisions for each split:\", precisions)\nprint(\"Recalls for each split:\", recalls)\n\n# Optionally, calculate average performance across splits\nprint(\"Average accuracy:\", np.mean(accuracies))\nprint(\"Average precision:\", np.mean(precisions))\nprint(\"Average recall:\", np.mean(recalls))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}